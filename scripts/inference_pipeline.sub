container_image = file:///staging/groups/glbrc_alphafold/af3/alphafold3.minimal.22Jan2025.sif

executable = inference_pipeline.sh

environment = "myjobdir=$(directory)"

MODEL_WEIGHTS_PATH = /staging/damorales4/af3/weights/af3.bin.zst

log = ../logs/inference_pipeline.log
output = inference_pipeline_$(Cluster)_$(Process).out
error  = inference_pipeline_$(Cluster)_$(Process).err

initialdir = $(directory)
# transfer all files in the inference_inputs directory
transfer_input_files = inference_inputs/, $(MODEL_WEIGHTS_PATH)

should_transfer_files = YES
when_to_transfer_output = ON_EXIT

request_memory = 8GB
# need space for the container (3GB) as well
request_disk = 10GB
request_cpus = 4
request_gpus = 1

# we should be able to run short jobs on CUDA_CAPABILITY=7.x but need
# other environment variables and options to be set. This is done automatically
# in inference_pipeline.sh
gpus_minimum_memory = 0

# short jobs 4-6 hours so it is okay to use is_resumable
+GPUJobLength = "short"
+WantGPULab = true
+is_resumable = true

# Use --user-specified-alphafold-options to pass any extra options to AlphaFold3, such as
# arguments = --model_param_file af3.bin.zst --work_dir_ext $(Cluster)_$(Process) --user-specified-alphafold-options "--buckets 5982"
arguments = --model_param_file af3.bin.zst --work_dir_ext $(Cluster)_$(Process)

queue directory from list_of_af3_jobs.txt
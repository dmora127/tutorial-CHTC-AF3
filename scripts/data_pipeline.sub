# CHTC maintained container for AlphaFold3 as of December 2025
container_image = file:///staging/groups/glbrc_alphafold/af3/alphafold3.minimal.22Jan2025.sif

executable = scripts/data_pipeline.sh

log = ./logs/data_pipeline.log
output = data_pipeline_$(Cluster)_$(Process).out
error  = data_pipeline_$(Cluster)_$(Process).err

initialdir = $(directory)
transfer_input_files = data_inputs/

should_transfer_files = YES
when_to_transfer_output = ON_EXIT

# transfer output files back to the submit node
transfer_output_files = $(directory).data_pipeline.tar.gz
transfer_output_remaps = "$(directory).data_pipeline.tar.gz=inference_inputs/$(directory).data_pipeline.tar.gz"

# We need this to transfer the databases to the execute node
# Requirements = (Target.HasCHTCStaging == true) && (Target.HasAlphafold3 == true)

if defined USE_SMALL_DB
  # testing requirements
  request_disk = 16GB
  arguments = --smalldb --work_dir_ext $(Cluster)_$(Process) --verbose
else
  # full requirements
  # Request less disk if matched machine already has AF3 DB preloaded (700GB savings)
  request_disk = 750000 - ( (TARGET.HasAlphafold3?: 1) * 700000)
  arguments = --work_dir_ext $(Cluster)_$(Proc) 
endif

if defined GREEDY_MODE
  request_disk = 750000 - ( (TARGET.HasAlphafold3?: 1) * 700000)
  Requirements = (Target.HasCHTCStaging == true)
else
  Requirements = (Target.HasCHTCStaging == true) && (Target.HasAlphafold3 == true)
  request_disk = 50GB 
endif

request_memory = 8GB
request_cpus = 8

queue directory from list_of_af3_jobs.txt
